{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport pandas_profiling\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import IsolationForest\nimport statsmodels.api as sm\nimport seaborn as sns\nfrom sklearn import linear_model\nimport warnings\nwarnings.simplefilter(\"ignore\")\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_excel('/kaggle/input/online-sales/Kaggle Anonymized Data 3.27.12 v4.xlsx') #read in the training data \ndf.info()\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1. Pre-processing\n### Alter product features and target vairable in scope of the problem statement"},{"metadata":{"trusted":true},"cell_type":"code","source":"fix = df.iloc[:,0:12].replace(0, np.nan)\nfix.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fix['Total Period Sales'] = fix.iloc[:,0:12].sum(axis='columns')\nfix['Active Sales Months'] = fix.iloc[:,0:12].count(axis='columns')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fix = fix.drop(columns = fix.iloc[:,0:12])\nfix.head() #dependant vairable and generated time feature","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.drop(columns = df.iloc[:,0:12])\ndf = fix.join(df)\ndf.columns = [x.strip() for x in df.columns] #remove spaces in column names","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Launch Month'] = df['Date_4'].dt.month #another time feature in independant vairables\ndf = df.drop(columns = ['Date_4']) #no more use for this given date vairable","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Nan and Missing Columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"nan_columns = list(df.columns[df.isnull().any()])\nmissing_columns = list(df.columns[df.isin(['MISSING']).any()])\nrouge_columns = nan_columns + missing_columns\nrouge_columns[0:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[rouge_columns] = df[rouge_columns].replace('MISSING', np.nan) #replace NaN values with the median of the column\ndf[rouge_columns] = df[rouge_columns].replace(np.nan, df.median(axis=0)) #replace NaN values with the median of the column","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Identify True Binary Features\nAs stated in the beggining, a product categorical feature can either be 0 (it does not possess the categorical feature) or 1 (it does possess the categorical feature). On close inspection of the dataframe above, you will notice there are some categorical measures > 1 or != 0. Upon consultation with the data source in [this thread](https://www.kaggle.com/c/online-sales/discussion/1898) - we can conclude and interpret that any vairable with a categorical feature that is not binary was either measured poorly. There are also quantitative vairables with categorical measures. \n\nWe will identify the binary measured features, and seperate them from the quantitative features. We will then normalize the quantitative columns, and remerge the two types of features to get our final independant vairables."},{"metadata":{"trusted":true},"cell_type":"code","source":"true_binary_cols = [col for col in df if np.isin(df[col].unique(), [0, 1]).all()] \ntrue_binary_cols[0:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Address Outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = IsolationForest( behaviour = 'new', max_samples=100, random_state = 1, contamination= 'auto')\ndf['preds'] = clf.fit_predict(df.iloc[:,0:1])\ndf = df[df['preds'] == 1] #these are our outcomes that are not outliers\ndf = df.drop(columns = ['preds'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.iloc[:,1:548]\nY = df.iloc[:,0:1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Model Developement"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split (X, Y, test_size=0.3, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Selection\nWe will utalise an OLS Backwards Elimination method to only compose the model of statistically significant product features (independent vairables) on the sales outcome (dependent vairable). Thereafter, we will check for multi-collinear vairables and eliminate any discrepancies."},{"metadata":{"trusted":true},"cell_type":"code","source":"y = y_train\ncols = list(x_train.columns)\npmax = 1\nwhile (len(cols)>0):\n    '''This automates the backwards elimination OLS method'''\n    p = []\n    X_1 = x_train[cols]\n    X_1 = sm.add_constant(X_1)\n    model = sm.OLS(y,X_1).fit()\n    p = pd.Series(model.pvalues.values[0:],index = cols)      \n    pmax = max(p)\n    feature_with_p_max = p.idxmax()\n    if(pmax>0.05):\n        cols.remove(feature_with_p_max)\n    else:\n        break\nselected_features_BE = cols\nprint(\"Product features that have statistical significance on the outcome vairable: \" + str(selected_features_BE))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = x_train.filter(items=selected_features_BE, axis=1) #our significant independant vairables only","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Check for Multi-Coliniarity"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compute the correlation matrix\ncorr = x_train.corr()\n\n# Generate heatmap\nsns.heatmap(corr, center=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On the forefront from the above heat map, we can see that some features are correlated with each other."},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.stats.outliers_influence import variance_inflation_factor\n[variance_inflation_factor(x_train.values, j) for j in range(x_train.shape[1])]\n\ndef calculate_vif(x):\n    thresh = 10.0\n    output = pd.DataFrame()\n    k = x.shape[1]\n    vif = [variance_inflation_factor(x.values, j) for j in range(x.shape[1])]\n    for i in range(1,k):\n        a = np.argmax(vif)\n        if vif[a] <= thresh :\n            break\n        if i == 1 :          \n            output = x.drop(x.columns[a], axis = 1)\n            vif = [variance_inflation_factor(output.values, j) for j in range(output.shape[1])]\n        elif i > 1 :\n            output = output.drop(output.columns[a],axis = 1)\n            vif = [variance_inflation_factor(output.values, j) for j in range(output.shape[1])]\n    return(output)\n\nselected_features = calculate_vif(x_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = selected_features\nx_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test = x_test.filter(items=x_train.columns, axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Test Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"regr = linear_model.LinearRegression()\nx = np.asanyarray(x_train)\ny = np.asanyarray(y_train)\nregr.fit (x, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_hat= regr.predict(x_test)\ny_hat = np.where(y_hat<0, 0, y_hat)\ny_hat = np.around(y_hat, decimals=0)\nx = np.asanyarray(x_test)\ny = np.asanyarray(y_test)\nprint(\"Residual sum of squares: %.2f\"\n% np.mean((y_hat - y) ** 2))\n# Explained variance score: 1 is perfect prediction\nprint('Variance score: %.2f' % regr.score(x, y))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A final accuracy of 95% is reported from our model (0.95 variance score). Let's see the results visualised below."},{"metadata":{"trusted":true},"cell_type":"code","source":"def DistributionPlot(y, yhat):\n    width = 12\n    height = 10\n    plt.figure(figsize=(width, height))\n    ax1 = sns.distplot(y, hist=True, color=\"r\", label='Actual')\n    ax2 = sns.distplot(yhat, hist=True, color=\"b\", label='Predicted', ax=ax1)\n    plt.title('Distribution Plot of Predicted Sales vs Actual Sales', size=16)\n    plt.legend()\n    plt.show()\n\nDistributionPlot(y, y_hat)","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3},"version":"3.7.4-final"},"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3,"kernelspec":{"name":"python37464bitbaseconda6239c13208d24200b375250a9805fd8e","display_name":"Python 3.7.4 64-bit ('base': conda)"}},"nbformat":4,"nbformat_minor":1}